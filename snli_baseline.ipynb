{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "import sys\n",
    "import unicodedata\n",
    "import string\n",
    "import operator\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "stdout = sys.stdout\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import os\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, dataDir = './'):\n",
    "        self.data = []\n",
    "        self.dataDir = dataDir \n",
    "\n",
    "    def isSampleValid(self, sample):\n",
    "        return sample['gold_label'] != '-'\n",
    "    \n",
    "    def getDataFromJSONL(self, filename):\n",
    "        numInvalid = 0\n",
    "        with open(filename) as fp:\n",
    "            reader = jsonlines.Reader(fp)\n",
    "            for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "                if self.isSampleValid(obj):\n",
    "                    sample = {}\n",
    "                    sample['gold_label'] = obj['gold_label']\n",
    "                    sample['sentence1'] = obj['sentence1']\n",
    "                    sample['sentence2'] = obj['sentence2']\n",
    "                    self.data.append(sample)\n",
    "                else:\n",
    "                    numInvalid+=1\n",
    "        print len(self.data)\n",
    "        print numInvalid\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n",
      "785\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "pp.getDataFromJSONL('./snli_1.0_train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/seasnake/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "stop = set(stopwords.words('english'))\n",
    "BLEU_Score_List = []\n",
    "for j in range(len(pp.data)):\n",
    "    weights = [1./4., 1./4., 1./4., 1./4.]\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    reference = [[i for i in sentence1.lower().split() if i not in stop]]\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    candidate = [i for i in sentence2.lower().split() if i not in stop]\n",
    "    length = min([len(reference[0]), len(candidate)]) \n",
    "    if length == 0:\n",
    "        BLEU_Score_List.append(0)\n",
    "    else:\n",
    "        if length < 4:\n",
    "            weights = ( 1. / length ,) * length\n",
    "        score = sentence_bleu(reference, candidate, weights)\n",
    "        BLEU_Score_List.append(score)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'BLEU_Score_List' (list) to file 'BLEU_Score_List.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store BLEU_Score_List > BLEU_Score_List.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BLEU_Score_List = eval(open(\"BLEU_Score_List.txt\").read())  \n",
    "BLEU_Score_Array = np.array(BLEU_Score_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BLEU_Score_Array = BLEU_Score_Array[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Target_List = [pp.data[j]['gold_label'] for j in range(len(pp.data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Target_List' (list) to file 'Target_List.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Target_List > Target_List.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Target_List = eval(open(\"Target_List.txt\").read())  \n",
    "Target_Array = np.array(Target_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367,)\n"
     ]
    }
   ],
   "source": [
    "print Target_Array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 1)\n"
     ]
    }
   ],
   "source": [
    "print BLEU_Score_Array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9824\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "tt = Preprocessor()\n",
    "tt.getDataFromJSONL('./snli_1.0_test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "stop = set(stopwords.words('english'))\n",
    "BLEU_Score_List_Test = []\n",
    "for j in range(len(tt.data)):\n",
    "    weights = [1./4., 1./4., 1./4., 1./4.]\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    reference = [[i for i in sentence1.lower().split() if i not in stop]]\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    candidate = [i for i in sentence2.lower().split() if i not in stop]\n",
    "    length = min([len(reference[0]), len(candidate)]) \n",
    "    if length == 0:\n",
    "        BLEU_Score_List_Test.append(0)\n",
    "    else:\n",
    "        if length < 4:\n",
    "            weights = ( 1. / length ,) * length\n",
    "        score = sentence_bleu(reference, candidate, weights)\n",
    "        BLEU_Score_List_Test.append(score)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'BLEU_Score_List_Test' (list) to file 'BLEU_Score_List_Test.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store BLEU_Score_List_Test > BLEU_Score_List_Test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BLEU_Score_List_Test = eval(open(\"BLEU_Score_List_Test.txt\").read())  \n",
    "BLEU_Score_Array_Test = np.array(BLEU_Score_List_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BLEU_Score_Array_Test = BLEU_Score_Array_Test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Target_List_Test = [tt.data[j]['gold_label'] for j in range(len(tt.data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Target_List_Test' (list) to file 'Target_List_Test.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Target_List_Test > Target_List_Test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Target_List_Test = eval(open(\"Target_List_Test.txt\").read())\n",
    "Target_Array_Test = np.array(Target_List_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list' (list) to file 'y_pred_list.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(BLEU_Score_Array, Target_Array).predict(BLEU_Score_Array_Test)\n",
    "y_pred_list = y_pred.tolist()\n",
    "%store y_pred_list > y_pred_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5957\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (BLEU_Score_Array_Test.shape[0],(Target_Array_Test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3867\n"
     ]
    }
   ],
   "source": [
    "print 9824 - 5957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.393627850163\n"
     ]
    }
   ],
   "source": [
    "print 3867./9824."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_mnb' (list) to file 'y_pred_list_mnb.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "y_pred_mnb = mnb.fit(BLEU_Score_Array, Target_Array).predict(BLEU_Score_Array_Test)\n",
    "y_pred_list_mnb = y_pred_mnb.tolist()\n",
    "%store y_pred_list_mnb > y_pred_list_mnb.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 6456\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (BLEU_Score_Array_Test.shape[0],(Target_Array_Test != y_pred_mnb).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3368\n"
     ]
    }
   ],
   "source": [
    "print 9824 - 6456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.342833876221\n"
     ]
    }
   ],
   "source": [
    "print 3368./9824."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm' (list) to file 'y_pred_list_svm.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_svm = clf.fit(BLEU_Score_Array, Target_Array).predict(BLEU_Score_Array_Test)\n",
    "y_pred_list_svm = y_pred_svm.tolist()\n",
    "%store y_pred_list_svm > y_pred_list_svm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 6587\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (BLEU_Score_Array_Test.shape[0],(Target_Array_Test != y_pred_svm).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3237\n"
     ]
    }
   ],
   "source": [
    "print 9824 - 6587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.329499185668\n"
     ]
    }
   ],
   "source": [
    "print 3237./9824."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.38      0.44      0.41      3237\n",
      " entailment       0.39      0.35      0.37      3368\n",
      "    neutral       0.41      0.39      0.40      3219\n",
      "\n",
      "avg / total       0.39      0.39      0.39      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.00      0.00      0.00      3237\n",
      " entailment       0.34      1.00      0.51      3368\n",
      "    neutral       0.00      0.00      0.00      3219\n",
      "\n",
      "avg / total       0.12      0.34      0.18      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.33      1.00      0.50      3237\n",
      " entailment       0.00      0.00      0.00      3368\n",
      "    neutral       0.00      0.00      0.00      3219\n",
      "\n",
      "avg / total       0.11      0.33      0.16      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1434,  919,  884],\n",
       "       [1253, 1192,  923],\n",
       "       [1062,  916, 1241]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Length_Difference_List = []\n",
    "for j in range(len(pp.data)):\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    premise = [i for i in sentence1.lower().split()]\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    hypothesis = [i for i in sentence2.lower().split()]\n",
    "    length_diff = len(premise)-len(hypothesis)\n",
    "    Length_Difference_List.append(length_diff)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Length_Difference_List' (list) to file 'Length_Difference_List.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Length_Difference_List > Length_Difference_List.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Length_Difference_List = eval(open(\"Length_Difference_List.txt\").read())  \n",
    "Length_Difference_Array = np.array(Length_Difference_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Length_Difference_Array = Length_Difference_Array[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Two_Features = np.concatenate((BLEU_Score_Array, Length_Difference_Array), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 2)\n"
     ]
    }
   ],
   "source": [
    "print Two_Features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Length_Difference_List_Test = []\n",
    "for j in range(len(tt.data)):\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    premise = [i for i in sentence1.lower().split()]\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    hypothesis = [i for i in sentence2.lower().split()]\n",
    "    length_diff = len(premise)-len(hypothesis)\n",
    "    Length_Difference_List_Test.append(length_diff)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Length_Difference_List_Test' (list) to file 'Length_Difference_List_Test.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Length_Difference_List_Test > Length_Difference_List_Test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Length_Difference_List_Test = eval(open(\"Length_Difference_List_Test.txt\").read())\n",
    "Length_Difference_Array_Test = np.array(Length_Difference_List_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Length_Difference_Array_Test = Length_Difference_Array_Test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Two_Features_Test = np.concatenate((BLEU_Score_Array_Test, Length_Difference_Array_Test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9824, 2)\n"
     ]
    }
   ],
   "source": [
    "print Two_Features_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_2_list' (list) to file 'y_pred_2_list.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_2 = gnb.fit(Two_Features, Target_Array).predict(Two_Features_Test)\n",
    "y_pred_2_list = y_pred_2.tolist()\n",
    "%store y_pred_2_list > y_pred_2_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5897\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Two_Features_Test.shape[0],(Target_Array_Test != y_pred_2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3927\n"
     ]
    }
   ],
   "source": [
    "print 9824 - 5897"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39973534202\n"
     ]
    }
   ],
   "source": [
    "print 3927./9824."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.42      0.21      0.28      3237\n",
      " entailment       0.39      0.59      0.47      3368\n",
      "    neutral       0.40      0.40      0.40      3219\n",
      "\n",
      "avg / total       0.41      0.40      0.38      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 677, 1624,  936],\n",
       "       [ 434, 1977,  957],\n",
       "       [ 490, 1456, 1273]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_2' (list) to file 'y_pred_list_svm_2.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_2_svm = clf.fit(Two_Features, Target_Array).predict(Two_Features_Test)\n",
    "y_pred_list_svm_2 = y_pred_2_svm.tolist()\n",
    "%store y_pred_list_svm_2 > y_pred_list_svm_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 6005\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Two_Features_Test.shape[0],(Target_Array_Test != y_pred_2_svm).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3819\n"
     ]
    }
   ],
   "source": [
    "print 9824 - 6005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.388741856678\n"
     ]
    }
   ],
   "source": [
    "print 3819./9824."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.44      0.04      0.08      3237\n",
      " entailment       0.37      0.89      0.52      3368\n",
      "    neutral       0.49      0.21      0.29      3219\n",
      "\n",
      "avg / total       0.43      0.39      0.30      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_2_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 136, 2688,  413],\n",
       "       [  71, 3013,  284],\n",
       "       [ 105, 2444,  670]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred_2_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "Overlap_Percent = []\n",
    "for j in range(len(pp.data)):\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    lengtharray = np.array([len(premise), len(hypothesis)])\n",
    "    length = np.min(lengtharray)\n",
    "    indx = np.argmin(lengtharray)\n",
    "    if length == 0:\n",
    "        Overlap_Percent.append(0)\n",
    "    else:\n",
    "        count = 0\n",
    "        if indx == 0:\n",
    "            for j in range(length):\n",
    "                if premise[j] in hypothesis:\n",
    "                    count = count+1\n",
    "        else:\n",
    "            for j in range(length):\n",
    "                if hypothesis[j] in premise:\n",
    "                    count = count+1\n",
    "        Overlap_Percent.append(count/float(length))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Overlap_Percent' (list) to file 'Overlap_Percent.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Overlap_Percent > Overlap_Percent.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Overlap_Percent_Array = np.array(Overlap_Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Overlap_Percent_Array = Overlap_Percent_Array[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "Overlap_Percent_Test = []\n",
    "for j in range(len(tt.data)):\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    lengtharray = np.array([len(premise), len(hypothesis)])\n",
    "    length = np.min(lengtharray)\n",
    "    indx = np.argmin(lengtharray)\n",
    "    if length == 0:\n",
    "        Overlap_Percent_Test.append(0)\n",
    "    else:\n",
    "        count = 0\n",
    "        if indx == 0:\n",
    "            for j in range(length):\n",
    "                if premise[j] in hypothesis:\n",
    "                    count = count+1\n",
    "        else:\n",
    "            for j in range(length):\n",
    "                if hypothesis[j] in premise:\n",
    "                    count = count+1\n",
    "        Overlap_Percent_Test.append(count/float(length))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Overlap_Percent_Test' (list) to file 'Overlap_Percent_Test.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Overlap_Percent_Test > Overlap_Percent_Test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Overlap_Percent_Array_Test = np.array(Overlap_Percent_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Overlap_Percent_Array_Test = Overlap_Percent_Array_Test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Three_Features = np.concatenate((Two_Features, Overlap_Percent_Array), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 3)\n"
     ]
    }
   ],
   "source": [
    "print Three_Features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Three_Features_Test = np.concatenate((Two_Features_Test, Overlap_Percent_Array_Test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9824, 3)\n"
     ]
    }
   ],
   "source": [
    "print Three_Features_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_3_list' (list) to file 'y_pred_3_list.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_3 = gnb.fit(Three_Features, Target_Array).predict(Three_Features_Test)\n",
    "y_pred_3_list = y_pred_3.tolist()\n",
    "%store y_pred_3_list > y_pred_3_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5174\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Two_Features_Test.shape[0],(Target_Array_Test != y_pred_3).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4650\n"
     ]
    }
   ],
   "source": [
    "print 9824 - 5174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.473330618893\n"
     ]
    }
   ],
   "source": [
    "print 4650./9824."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.43      0.54      0.48      3237\n",
      " entailment       0.55      0.54      0.55      3368\n",
      "    neutral       0.43      0.34      0.38      3219\n",
      "\n",
      "avg / total       0.47      0.47      0.47      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1735,  680,  822],\n",
       "       [ 937, 1823,  608],\n",
       "       [1340,  787, 1092]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_3' (list) to file 'y_pred_list_svm_3.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_3_svm = clf.fit(Three_Features, Target_Array).predict(Three_Features_Test)\n",
    "y_pred_list_svm_3 = y_pred_3_svm.tolist()\n",
    "%store y_pred_list_svm_3 > y_pred_list_svm_3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5343\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Three_Features_Test.shape[0],(Target_Array_Test != y_pred_3_svm).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4481\n"
     ]
    }
   ],
   "source": [
    "print 9824 - 5343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.456127850163\n"
     ]
    }
   ],
   "source": [
    "print 4481./9824."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.42      0.67      0.52      3237\n",
      " entailment       0.50      0.63      0.56      3368\n",
      "    neutral       0.47      0.06      0.11      3219\n",
      "\n",
      "avg / total       0.46      0.46      0.40      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_3_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2168,  938,  131],\n",
       "       [1160, 2110,   98],\n",
       "       [1851, 1165,  203]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred_3_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
