{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import sys\n",
    "import unicodedata\n",
    "import string\n",
    "import operator\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "stdout = sys.stdout\n",
    "importlib.reload(sys)  \n",
    "#sys.setdefaultencoding('utf8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import os\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, dataDir = './'):\n",
    "        self.data = []\n",
    "        self.dataDir = dataDir \n",
    "\n",
    "    def isSampleValid(self, sample):\n",
    "        return sample['gold_label'] != '-'\n",
    "    \n",
    "    def getDataFromJSONL(self, filename):\n",
    "        numInvalid = 0\n",
    "        with open(filename) as fp:\n",
    "            reader = jsonlines.Reader(fp)\n",
    "            for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "                if self.isSampleValid(obj):\n",
    "                    sample = {}\n",
    "                    sample['gold_label'] = obj['gold_label']\n",
    "                    sample['sentence1'] = obj['sentence1']\n",
    "                    sample['sentence2'] = obj['sentence2']\n",
    "                    self.data.append(sample)\n",
    "                else:\n",
    "                    numInvalid+=1\n",
    "        print (len(self.data))\n",
    "        print (numInvalid)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n",
      "785\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "pp.getDataFromJSONL('./snli_1.0/snli_1.0_train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "stop = set(stopwords.words('english'))\n",
    "BLEU_Score_List = []\n",
    "for j in range(len(pp.data)):\n",
    "    weights = [1./4., 1./4., 1./4., 1./4.]\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    reference = [[i for i in sentence1.lower().split() if i not in stop]]\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    candidate = [i for i in sentence2.lower().split() if i not in stop]\n",
    "    length = min([len(reference[0]), len(candidate)]) \n",
    "    if length == 0:\n",
    "        BLEU_Score_List.append(0)\n",
    "    else:\n",
    "        if length < 4:\n",
    "            weights = ( 1. / length ,) * length\n",
    "        score = sentence_bleu(reference, candidate, weights)\n",
    "        BLEU_Score_List.append(score)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'BLEU_Score_List' (list) to file 'BLEU_Score_List.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store BLEU_Score_List > BLEU_Score_List.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU_Score_List = eval(open(\"BLEU_Score_List.txt\").read())  \n",
    "BLEU_Score_Array = np.array(BLEU_Score_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU_Score_Array = BLEU_Score_Array[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_List = [pp.data[j]['gold_label'] for j in range(len(pp.data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Target_List' (list) to file 'Target_List.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Target_List > Target_List.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_List = eval(open(\"Target_List.txt\").read())  \n",
    "Target_Array = np.array(Target_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367,)\n"
     ]
    }
   ],
   "source": [
    "print (Target_Array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 1)\n",
      "[[5.79888111e-155]\n",
      " [5.39646693e-155]\n",
      " [5.56498649e-155]\n",
      " [1.38429296e-231]\n",
      " [7.81350843e-232]\n",
      " [0.00000000e+000]\n",
      " [4.31073290e-232]\n",
      " [4.77061016e-232]\n",
      " [2.61213321e-155]\n",
      " [1.52692335e-078]]\n"
     ]
    }
   ],
   "source": [
    "print (BLEU_Score_Array.shape)\n",
    "print (BLEU_Score_Array[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n",
      "785\n"
     ]
    }
   ],
   "source": [
    "vv = Preprocessor()\n",
    "vv.getDataFromJSONL('./snli_1.0/snli_1.0_dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "stop = set(stopwords.words('english'))\n",
    "BLEU_Score_List_Val = []\n",
    "for j in range(len(vv.data)):\n",
    "    weights = [1./4., 1./4., 1./4., 1./4.]\n",
    "    sentence1 = vv.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    reference = [[i for i in sentence1.lower().split() if i not in stop]]\n",
    "    sentence2 = vv.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    candidate = [i for i in sentence2.lower().split() if i not in stop]\n",
    "    length = min([len(reference[0]), len(candidate)]) \n",
    "    if length == 0:\n",
    "        BLEU_Score_List_Val.append(0)\n",
    "    else:\n",
    "        if length < 4:\n",
    "            weights = ( 1. / length ,) * length\n",
    "        score = sentence_bleu(reference, candidate, weights)\n",
    "        BLEU_Score_List_Val.append(score)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU_Score_Array_Test = np.array(BLEU_Score_List_Test)\n",
    "BLEU_Score_Array_Test = BLEU_Score_Array_Test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9824\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "tt = Preprocessor()\n",
    "tt.getDataFromJSONL('./snli_1.0/snli_1.0_test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "stop = set(stopwords.words('english'))\n",
    "BLEU_Score_List_Test = []\n",
    "for j in range(len(tt.data)):\n",
    "    weights = [1./4., 1./4., 1./4., 1./4.]\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    reference = [[i for i in sentence1.lower().split() if i not in stop]]\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    candidate = [i for i in sentence2.lower().split() if i not in stop]\n",
    "    length = min([len(reference[0]), len(candidate)]) \n",
    "    if length == 0:\n",
    "        BLEU_Score_List_Test.append(0)\n",
    "    else:\n",
    "        if length < 4:\n",
    "            weights = ( 1. / length ,) * length\n",
    "        score = sentence_bleu(reference, candidate, weights)\n",
    "        BLEU_Score_List_Test.append(score)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'BLEU_Score_List_Test' (list) to file 'BLEU_Score_List_Test.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store BLEU_Score_List_Test > BLEU_Score_List_Test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU_Score_List_Test = eval(open(\"BLEU_Score_List_Test.txt\").read())  \n",
    "BLEU_Score_Array_Test = np.array(BLEU_Score_List_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU_Score_Array_Test = BLEU_Score_Array_Test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_List_Test = [tt.data[j]['gold_label'] for j in range(len(tt.data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Target_List_Test' (list) to file 'Target_List_Test.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Target_List_Test > Target_List_Test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_List_Test = eval(open(\"Target_List_Test.txt\").read())\n",
    "Target_Array_Test = np.array(Target_List_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_train' (list) to file 'y_pred_list_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_train = gnb.fit(BLEU_Score_Array, Target_Array).predict(BLEU_Score_Array)\n",
    "y_pred_list_train = y_pred_train.tolist()\n",
    "%store y_pred_list_train > y_pred_list_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 358144\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (BLEU_Score_Array.shape[0],(Target_Array != y_pred_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217742\n"
     ]
    }
   ],
   "source": [
    "print (549367 - 331625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.396350709088824\n"
     ]
    }
   ],
   "source": [
    "print (217742./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list' (list) to file 'y_pred_list.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(BLEU_Score_Array, Target_Array).predict(BLEU_Score_Array_Test)\n",
    "y_pred_list = y_pred.tolist()\n",
    "%store y_pred_list > y_pred_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 6367\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (BLEU_Score_Array_Test.shape[0],(Target_Array_Test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3867\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 5957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39362785016286644\n"
     ]
    }
   ],
   "source": [
    "print (3867./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_mnb' (list) to file 'y_pred_list_mnb.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "y_pred_mnb = mnb.fit(BLEU_Score_Array, Target_Array).predict(BLEU_Score_Array_Test)\n",
    "y_pred_list_mnb = y_pred_mnb.tolist()\n",
    "%store y_pred_list_mnb > y_pred_list_mnb.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 6456\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (BLEU_Score_Array_Test.shape[0],(Target_Array_Test != y_pred_mnb).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3368\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 6456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34283387622149836\n"
     ]
    }
   ],
   "source": [
    "print (3368./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_train' (list) to file 'y_pred_list_svm_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_svm_train = clf.fit(BLEU_Score_Array, Target_Array).predict(BLEU_Score_Array)\n",
    "y_pred_list_svm_train = y_pred_svm_train.tolist()\n",
    "%store y_pred_list_svm_train > y_pred_list_svm_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 366603\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (BLEU_Score_Array.shape[0],(Target_Array != y_pred_svm_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187686\n"
     ]
    }
   ],
   "source": [
    "print (549367 - 361681)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34164046984984536\n"
     ]
    }
   ],
   "source": [
    "print (187686./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm' (list) to file 'y_pred_list_svm.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_svm = clf.fit(BLEU_Score_Array, Target_Array).predict(BLEU_Score_Array_Test)\n",
    "y_pred_list_svm = y_pred_svm.tolist()\n",
    "%store y_pred_list_svm > y_pred_list_svm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 6473\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (BLEU_Score_Array_Test.shape[0],(Target_Array_Test != y_pred_svm).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3237\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 6587)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32949918566775244\n"
     ]
    }
   ],
   "source": [
    "print (3237./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_lr_train' (list) to file 'y_pred_list_lr_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_lr_train = clf.fit(BLEU_Score_Array, Target_Array).predict(BLEU_Score_Array)\n",
    "y_pred_list_lr_train = y_pred_lr_train.tolist()\n",
    "%store y_pred_list_lr_train > y_pred_list_lr_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 356679\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (BLEU_Score_Array.shape[0],(Target_Array != y_pred_lr_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204739\n"
     ]
    }
   ],
   "source": [
    "print (549367 - 344628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3726816499716947\n"
     ]
    }
   ],
   "source": [
    "print (204739./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_lr' (list) to file 'y_pred_list_lr.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_lr = clf.fit(BLEU_Score_Array, Target_Array).predict(BLEU_Score_Array_Test)\n",
    "y_pred_list_lr = y_pred_lr.tolist()\n",
    "%store y_pred_list_lr > y_pred_list_lr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 6330\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (BLEU_Score_Array_Test.shape[0],(Target_Array_Test != y_pred_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3626\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 6198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36909609120521175\n"
     ]
    }
   ],
   "source": [
    "print (3626./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.34      0.94      0.50      3237\n",
      "   entailment       0.47      0.12      0.19      3368\n",
      "      neutral       0.00      0.00      0.00      3219\n",
      "\n",
      "     accuracy                           0.35      9824\n",
      "    macro avg       0.27      0.35      0.23      9824\n",
      " weighted avg       0.27      0.35      0.23      9824\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.00      0.00      0.00      3237\n",
      "   entailment       0.34      1.00      0.51      3368\n",
      "      neutral       0.00      0.00      0.00      3219\n",
      "\n",
      "     accuracy                           0.34      9824\n",
      "    macro avg       0.11      0.33      0.17      9824\n",
      " weighted avg       0.12      0.34      0.18      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.00      0.00      0.00      3237\n",
      "   entailment       0.47      0.12      0.19      3368\n",
      "      neutral       0.33      0.91      0.48      3219\n",
      "\n",
      "     accuracy                           0.34      9824\n",
      "    macro avg       0.26      0.35      0.23      9824\n",
      " weighted avg       0.27      0.34      0.22      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.34      0.93      0.50      3237\n",
      "   entailment       0.48      0.14      0.22      3368\n",
      "      neutral       0.00      0.00      0.00      3219\n",
      "\n",
      "     accuracy                           0.36      9824\n",
      "    macro avg       0.27      0.36      0.24      9824\n",
      " weighted avg       0.28      0.36      0.24      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3044,  193,    0],\n",
       "       [2955,  413,    0],\n",
       "       [2938,  281,    0]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3017,  220,    0],\n",
       "       [2891,  477,    0],\n",
       "       [2924,  295,    0]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Length_Difference_List = []\n",
    "for j in range(len(pp.data)):\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    premise = [i for i in sentence1.lower().split()]\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    hypothesis = [i for i in sentence2.lower().split()]\n",
    "    length_diff = len(premise)-len(hypothesis)\n",
    "    Length_Difference_List.append(length_diff)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Length_Difference_List' (list) to file 'Length_Difference_List.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Length_Difference_List > Length_Difference_List.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Length_Difference_List = eval(open(\"Length_Difference_List.txt\").read())  \n",
    "Length_Difference_Array = np.array(Length_Difference_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Length_Difference_Array = Length_Difference_Array[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Two_Features = np.concatenate((BLEU_Score_Array, Length_Difference_Array), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 2)\n"
     ]
    }
   ],
   "source": [
    "print (Two_Features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Length_Difference_List_Test = []\n",
    "for j in range(len(tt.data)):\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    premise = [i for i in sentence1.lower().split()]\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2).encode('ascii','ignore')\n",
    "    hypothesis = [i for i in sentence2.lower().split()]\n",
    "    length_diff = len(premise)-len(hypothesis)\n",
    "    Length_Difference_List_Test.append(length_diff)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Length_Difference_List_Test' (list) to file 'Length_Difference_List_Test.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Length_Difference_List_Test > Length_Difference_List_Test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Length_Difference_List_Test = eval(open(\"Length_Difference_List_Test.txt\").read())\n",
    "Length_Difference_Array_Test = np.array(Length_Difference_List_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Length_Difference_Array_Test = Length_Difference_Array_Test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Two_Features_Test = np.concatenate((BLEU_Score_Array_Test, Length_Difference_Array_Test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9824, 2)\n"
     ]
    }
   ],
   "source": [
    "print (Two_Features_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_2_list_train' (list) to file 'y_pred_2_list_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_2_train = gnb.fit(Two_Features, Target_Array).predict(Two_Features)\n",
    "y_pred_2_list_train = y_pred_2_train.tolist()\n",
    "%store y_pred_2_list_train > y_pred_2_list_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 343682\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Two_Features.shape[0],(Target_Array != y_pred_2_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219634\n"
     ]
    }
   ],
   "source": [
    "print (549367 - 329733)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3997946727779426\n"
     ]
    }
   ],
   "source": [
    "print (219634./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_2_list' (list) to file 'y_pred_2_list.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_2 = gnb.fit(Two_Features, Target_Array).predict(Two_Features_Test)\n",
    "y_pred_2_list = y_pred_2.tolist()\n",
    "%store y_pred_2_list > y_pred_2_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 6094\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Two_Features_Test.shape[0],(Target_Array_Test != y_pred_2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3927\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 5897)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39973534201954397\n"
     ]
    }
   ],
   "source": [
    "print (3927./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.34      0.85      0.49      3237\n",
      "   entailment       0.55      0.12      0.20      3368\n",
      "      neutral       0.52      0.18      0.27      3219\n",
      "\n",
      "     accuracy                           0.38      9824\n",
      "    macro avg       0.47      0.38      0.32      9824\n",
      " weighted avg       0.47      0.38      0.32      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2749,  166,  322],\n",
       "       [2749,  400,  219],\n",
       "       [2477,  161,  581]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_2_train' (list) to file 'y_pred_list_svm_2_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_2_svm_train = clf.fit(Two_Features, Target_Array).predict(Two_Features)\n",
    "y_pred_list_svm_2_train = y_pred_2_svm_train.tolist()\n",
    "%store y_pred_list_svm_2_train > y_pred_list_svm_2_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 353298\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Two_Features.shape[0],(Target_Array != y_pred_2_svm_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212490\n"
     ]
    }
   ],
   "source": [
    "print (549367 - 336877)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3867906153809748\n"
     ]
    }
   ],
   "source": [
    "print (212490./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_2' (list) to file 'y_pred_list_svm_2.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_2_svm = clf.fit(Two_Features, Target_Array).predict(Two_Features_Test)\n",
    "y_pred_list_svm_2 = y_pred_2_svm.tolist()\n",
    "%store y_pred_list_svm_2 > y_pred_list_svm_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 6255\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Two_Features_Test.shape[0],(Target_Array_Test != y_pred_2_svm).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3819\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 6005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3887418566775244\n"
     ]
    }
   ],
   "source": [
    "print (3819./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.28      0.09      0.13      3237\n",
      "   entailment       0.37      0.87      0.52      3368\n",
      "      neutral       0.37      0.11      0.17      3219\n",
      "\n",
      "     accuracy                           0.36      9824\n",
      "    macro avg       0.34      0.36      0.27      9824\n",
      " weighted avg       0.34      0.36      0.28      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_2_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 277, 2608,  352],\n",
       "       [ 171, 2942,  255],\n",
       "       [ 533, 2336,  350]], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred_2_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_lr_2_train' (list) to file 'y_pred_list_lr_2_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_2_lr_train = clf.fit(Two_Features, Target_Array).predict(Two_Features)\n",
    "y_pred_list_lr_2_train = y_pred_2_lr_train.tolist()\n",
    "%store y_pred_list_lr_2_train > y_pred_list_lr_2_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 343526\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Two_Features.shape[0],(Target_Array != y_pred_2_lr_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212084\n"
     ]
    }
   ],
   "source": [
    "print (549367 - 337283)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38605158300371156\n"
     ]
    }
   ],
   "source": [
    "print (212084./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_lr_2' (list) to file 'y_pred_list_lr_2.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_2_lr = clf.fit(Two_Features, Target_Array).predict(Two_Features_Test)\n",
    "y_pred_list_lr_2 = y_pred_2_lr.tolist()\n",
    "%store y_pred_list_lr_2 > y_pred_list_lr_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 6117\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Two_Features_Test.shape[0],(Target_Array_Test != y_pred_2_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 6029)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3862988599348534\n"
     ]
    }
   ],
   "source": [
    "print (3795./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.33      0.17      0.23      3237\n",
      "   entailment       0.39      0.48      0.43      3368\n",
      "      neutral       0.38      0.47      0.42      3219\n",
      "\n",
      "     accuracy                           0.38      9824\n",
      "    macro avg       0.37      0.38      0.36      9824\n",
      " weighted avg       0.37      0.38      0.36      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_2_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "Overlap_Percent = []\n",
    "for j in range(len(pp.data)):\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    #sentence1 = unicodedata.normalize('NFKD', sentence1).encode('ascii','ignore')\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1)\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    lengtharray = np.array([len(premise), len(hypothesis)])\n",
    "    length = np.min(lengtharray)\n",
    "    indx = np.argmin(lengtharray)\n",
    "    if length == 0:\n",
    "        Overlap_Percent.append(0)\n",
    "    else:\n",
    "        count = 0\n",
    "        if indx == 0:\n",
    "            for j in range(length):\n",
    "                if premise[j] in hypothesis:\n",
    "                    count = count+1\n",
    "        else:\n",
    "            for j in range(length):\n",
    "                if hypothesis[j] in premise:\n",
    "                    count = count+1\n",
    "        Overlap_Percent.append(count/float(length))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Overlap_Percent' (list) to file 'Overlap_Percent.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Overlap_Percent > Overlap_Percent.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overlap_Percent = eval(open(\"Overlap_Percent.txt\").read())\n",
    "Overlap_Percent_Array = np.array(Overlap_Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overlap_Percent_Array = Overlap_Percent_Array[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "Overlap_Percent_Test = []\n",
    "for j in range(len(tt.data)):\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1)\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    lengtharray = np.array([len(premise), len(hypothesis)])\n",
    "    length = np.min(lengtharray)\n",
    "    indx = np.argmin(lengtharray)\n",
    "    if length == 0:\n",
    "        Overlap_Percent_Test.append(0)\n",
    "    else:\n",
    "        count = 0\n",
    "        if indx == 0:\n",
    "            for j in range(length):\n",
    "                if premise[j] in hypothesis:\n",
    "                    count = count+1\n",
    "        else:\n",
    "            for j in range(length):\n",
    "                if hypothesis[j] in premise:\n",
    "                    count = count+1\n",
    "        Overlap_Percent_Test.append(count/float(length))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Overlap_Percent_Test' (list) to file 'Overlap_Percent_Test.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Overlap_Percent_Test > Overlap_Percent_Test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overlap_Percent_Test = eval(open(\"Overlap_Percent_Test.txt\").read())\n",
    "Overlap_Percent_Array_Test = np.array(Overlap_Percent_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overlap_Percent_Array_Test = Overlap_Percent_Array_Test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Three_Features = np.concatenate((Two_Features, Overlap_Percent_Array), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 3)\n"
     ]
    }
   ],
   "source": [
    "print (Three_Features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "Three_Features_Test = np.concatenate((Two_Features_Test, Overlap_Percent_Array_Test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9824, 3)\n"
     ]
    }
   ],
   "source": [
    "print (Three_Features_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_3_list_train' (list) to file 'y_pred_3_list_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_3_train = gnb.fit(Three_Features, Target_Array).predict(Three_Features)\n",
    "y_pred_3_list_train = y_pred_3_train.tolist()\n",
    "%store y_pred_3_list_train > y_pred_3_list_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 297560\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Three_Features.shape[0],(Target_Array != y_pred_3_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255721\n"
     ]
    }
   ],
   "source": [
    "print (549367-293646)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46548300134518455\n"
     ]
    }
   ],
   "source": [
    "print (255721./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_3_list' (list) to file 'y_pred_3_list.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_3 = gnb.fit(Three_Features, Target_Array).predict(Three_Features_Test)\n",
    "y_pred_3_list = y_pred_3.tolist()\n",
    "%store y_pred_3_list > y_pred_3_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5288\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Three_Features_Test.shape[0],(Target_Array_Test != y_pred_3).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4650\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 5174)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47333061889250816\n"
     ]
    }
   ],
   "source": [
    "print (4650./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.42      0.67      0.51      3237\n",
      "   entailment       0.55      0.52      0.54      3368\n",
      "      neutral       0.43      0.18      0.26      3219\n",
      "\n",
      "     accuracy                           0.46      9824\n",
      "    macro avg       0.47      0.46      0.44      9824\n",
      " weighted avg       0.47      0.46      0.44      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2183,  654,  400],\n",
       "       [1222, 1766,  380],\n",
       "       [1838,  794,  587]], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_3_train' (list) to file 'y_pred_list_svm_3_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_3_svm_train = clf.fit(Three_Features, Target_Array).predict(Three_Features)\n",
    "y_pred_list_svm_3_train = y_pred_3_svm_train.tolist()\n",
    "%store y_pred_list_svm_3_train > y_pred_list_svm_3_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 304885\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Three_Features.shape[0],(Target_Array != y_pred_3_svm_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234860\n"
     ]
    }
   ],
   "source": [
    "print (549367 - 314507)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42751020720210714\n"
     ]
    }
   ],
   "source": [
    "print (234860./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_3' (list) to file 'y_pred_list_svm_3.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_3_svm = clf.fit(Three_Features, Target_Array).predict(Three_Features_Test)\n",
    "y_pred_list_svm_3 = y_pred_3_svm.tolist()\n",
    "%store y_pred_list_svm_3 > y_pred_list_svm_3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5700\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Three_Features_Test.shape[0],(Target_Array_Test != y_pred_3_svm).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4520\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 5304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4600977198697068\n"
     ]
    }
   ],
   "source": [
    "print (4520./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.43      0.34      0.38      3237\n",
      "   entailment       0.42      0.80      0.55      3368\n",
      "      neutral       0.38      0.11      0.17      3219\n",
      "\n",
      "     accuracy                           0.42      9824\n",
      "    macro avg       0.41      0.41      0.37      9824\n",
      " weighted avg       0.41      0.42      0.37      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_3_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1088, 1774,  375],\n",
       "       [ 489, 2695,  184],\n",
       "       [ 931, 1947,  341]], dtype=int64)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred_3_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_lr_3_train' (list) to file 'y_pred_list_lr_3_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_3_lr_train = clf.fit(Three_Features, Target_Array).predict(Three_Features)\n",
    "y_pred_list_lr_3_train = y_pred_3_lr_train.tolist()\n",
    "%store y_pred_list_lr_3_train > y_pred_list_lr_3_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 295417\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Three_Features.shape[0],(Target_Array != y_pred_3_lr_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251972\n"
     ]
    }
   ],
   "source": [
    "print (549367 - 297395)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4586587836546425\n"
     ]
    }
   ],
   "source": [
    "print (251972./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_lr_3' (list) to file 'y_pred_list_lr_3.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_3_lr = clf.fit(Three_Features, Target_Array).predict(Three_Features_Test)\n",
    "y_pred_list_lr_3 = y_pred_3_lr.tolist()\n",
    "%store y_pred_list_lr_3 > y_pred_list_lr_3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5258\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Three_Features_Test.shape[0],(Target_Array_Test != y_pred_3_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4571\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 5253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46528908794788276\n"
     ]
    }
   ],
   "source": [
    "print (4571./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.44      0.53      0.48      3237\n",
      "   entailment       0.50      0.63      0.56      3368\n",
      "      neutral       0.43      0.23      0.30      3219\n",
      "\n",
      "     accuracy                           0.46      9824\n",
      "    macro avg       0.46      0.46      0.45      9824\n",
      " weighted avg       0.46      0.46      0.45      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_3_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.44      0.53      0.48      3237\n",
      "   entailment       0.50      0.63      0.56      3368\n",
      "      neutral       0.43      0.23      0.30      3219\n",
      "\n",
      "     accuracy                           0.46      9824\n",
      "    macro avg       0.46      0.46      0.45      9824\n",
      " weighted avg       0.46      0.46      0.45      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_3_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = pp.data[2]['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sid.polarity_scores(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.307, 'neu': 0.693, 'pos': 0.0, 'compound': -0.4767}\n"
     ]
    }
   ],
   "source": [
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person on a horse jumps over a broken down airplane.\n"
     ]
    }
   ],
   "source": [
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = pp.data[2]['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sid.polarity_scores(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person is outdoors, on a horse.\n"
     ]
    }
   ],
   "source": [
    "print (sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Premise_Sentiment' (list) to file 'Premise_Sentiment.txt'.\n",
      "Writing 'Hypothesis_Sentiment' (list) to file 'Hypothesis_Sentiment.txt'.\n",
      "Writing 'Match' (list) to file 'Match.txt'.\n"
     ]
    }
   ],
   "source": [
    "Premise_Sentiment = []\n",
    "Hypothesis_Sentiment = []\n",
    "Match = []\n",
    "for j in range(len(pp.data)):\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    premise_ss = sid.polarity_scores(sentence1)\n",
    "    Premise_Sentiment.append(premise_ss['compound'])\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    hypothesis_ss = sid.polarity_scores(sentence2)\n",
    "    Hypothesis_Sentiment.append(hypothesis_ss['compound'])\n",
    "    if premise_ss['compound'] >= 0 and hypothesis_ss['compound'] >= 0:\n",
    "        Match.append(1)\n",
    "    elif premise_ss['compound'] >=0 and hypothesis_ss['compound'] < 0:\n",
    "        Match.append(-1)\n",
    "    elif premise_ss['compound'] < 0 and hypothesis_ss['compound'] >= 0:\n",
    "        Match.append(-1)\n",
    "    else:\n",
    "        Match.append(1)\n",
    "%store Premise_Sentiment > Premise_Sentiment.txt\n",
    "%store Hypothesis_Sentiment > Hypothesis_Sentiment.txt\n",
    "%store Match > Match.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "Premise_Sentiment = eval(open(\"Premise_Sentiment.txt\").read())\n",
    "Premise_Sentiment_Array = np.array(Premise_Sentiment)\n",
    "Hypothesis_Sentiment = eval(open(\"Hypothesis_Sentiment.txt\").read())\n",
    "Hypothesis_Sentiment_Array = np.array(Hypothesis_Sentiment)\n",
    "Match = eval(open(\"Match.txt\").read())\n",
    "Match_Array = np.array(Match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "Premise_Sentiment_Array = Premise_Sentiment_Array[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hypothesis_Sentiment_Array = Hypothesis_Sentiment_Array[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "Match_Array = Match_Array[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "Six_Features = np.concatenate((Three_Features, Premise_Sentiment_Array, Hypothesis_Sentiment_Array, Match_Array), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 6)\n"
     ]
    }
   ],
   "source": [
    "print(Six_Features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Premise_Sentiment_Test' (list) to file 'Premise_Sentiment_Test.txt'.\n",
      "Writing 'Hypothesis_Sentiment_Test' (list) to file 'Hypothesis_Sentiment_Test.txt'.\n",
      "Writing 'Match_Test' (list) to file 'Match_Test.txt'.\n"
     ]
    }
   ],
   "source": [
    "Premise_Sentiment_Test = []\n",
    "Hypothesis_Sentiment_Test = []\n",
    "Match_Test = []\n",
    "for j in range(len(tt.data)):\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    premise_ss = sid.polarity_scores(sentence1)\n",
    "    Premise_Sentiment_Test.append(premise_ss['compound'])\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    hypothesis_ss = sid.polarity_scores(sentence2)\n",
    "    Hypothesis_Sentiment_Test.append(hypothesis_ss['compound'])\n",
    "    if premise_ss['compound'] >= 0 and hypothesis_ss['compound'] >= 0:\n",
    "        Match_Test.append(1)\n",
    "    elif premise_ss['compound'] >=0 and hypothesis_ss['compound'] < 0:\n",
    "        Match_Test.append(-1)\n",
    "    elif premise_ss['compound'] < 0 and hypothesis_ss['compound'] >= 0:\n",
    "        Match_Test.append(-1)\n",
    "    else:\n",
    "        Match_Test.append(1)\n",
    "%store Premise_Sentiment_Test > Premise_Sentiment_Test.txt\n",
    "%store Hypothesis_Sentiment_Test > Hypothesis_Sentiment_Test.txt\n",
    "%store Match_Test > Match_Test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "Premise_Sentiment_Test = eval(open(\"Premise_Sentiment_Test.txt\").read())\n",
    "Premise_Sentiment_Array_Test = np.array(Premise_Sentiment_Test)\n",
    "Hypothesis_Sentiment_Test = eval(open(\"Hypothesis_Sentiment_Test.txt\").read())\n",
    "Hypothesis_Sentiment_Array_Test = np.array(Hypothesis_Sentiment_Test)\n",
    "Match_Test = eval(open(\"Match_Test.txt\").read())\n",
    "Match_Array_Test = np.array(Match_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "Premise_Sentiment_Array_Test = Premise_Sentiment_Array_Test[:, np.newaxis]\n",
    "Hypothesis_Sentiment_Array_Test = Hypothesis_Sentiment_Array_Test[:, np.newaxis]\n",
    "Match_Array_Test = Match_Array_Test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "Six_Features_Test = np.concatenate((Three_Features_Test, Premise_Sentiment_Array_Test, Hypothesis_Sentiment_Array_Test, Match_Array_Test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9824, 6)\n"
     ]
    }
   ],
   "source": [
    "print (Six_Features_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_6_list' (list) to file 'y_pred_6_list.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_6 = gnb.fit(Six_Features, Target_Array).predict(Six_Features_Test)\n",
    "y_pred_6_list = y_pred_6.tolist()\n",
    "%store y_pred_6_list > y_pred_6_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5175\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Six_Features_Test.shape[0],(Target_Array_Test != y_pred_6).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_6' (list) to file 'y_pred_list_svm_6.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_6_svm = clf.fit(Six_Features, Target_Array).predict(Six_Features_Test)\n",
    "y_pred_list_svm_6 = y_pred_6_svm.tolist()\n",
    "%store y_pred_list_svm_6 > y_pred_list_svm_6.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5168\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Six_Features_Test.shape[0],(Target_Array_Test != y_pred_6_svm).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_lr_6' (list) to file 'y_pred_list_lr_6.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_6_lr = clf.fit(Six_Features, Target_Array).predict(Six_Features_Test)\n",
    "y_pred_list_lr_6 = y_pred_6_lr.tolist()\n",
    "%store y_pred_list_lr_6 > y_pred_list_lr_6.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5060\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Six_Features_Test.shape[0],(Target_Array_Test != y_pred_6_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4732\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 5092)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4816775244299674\n"
     ]
    }
   ],
   "source": [
    "print (4732./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "Five_Features = np.concatenate((Three_Features, Hypothesis_Sentiment_Array, Match_Array), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "Five_Features_Test = np.concatenate((Three_Features_Test, Hypothesis_Sentiment_Array_Test, Match_Array_Test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_5_list' (list) to file 'y_pred_5_list.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_5 = gnb.fit(Five_Features, Target_Array).predict(Five_Features_Test)\n",
    "y_pred_5_list = y_pred_5.tolist()\n",
    "%store y_pred_5_list > y_pred_5_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5184\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Five_Features_Test.shape[0],(Target_Array_Test != y_pred_5).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_5' (list) to file 'y_pred_list_svm_5.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_5_svm = clf.fit(Five_Features, Target_Array).predict(Five_Features_Test)\n",
    "y_pred_list_svm_5 = y_pred_5_svm.tolist()\n",
    "%store y_pred_list_svm_5 > y_pred_list_svm_5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5344\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Five_Features_Test.shape[0],(Target_Array_Test != y_pred_5_svm).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_lr_5' (list) to file 'y_pred_list_lr_5.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_5_lr = clf.fit(Five_Features, Target_Array).predict(Five_Features_Test)\n",
    "y_pred_list_lr_5 = y_pred_5_lr.tolist()\n",
    "%store y_pred_list_lr_5 > y_pred_list_lr_5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5078\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Five_Features_Test.shape[0],(Target_Array_Test != y_pred_5_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_Features_H = np.concatenate((Three_Features, Hypothesis_Sentiment_Array), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_Features_Test_H = np.concatenate((Three_Features_Test, Hypothesis_Sentiment_Array_Test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_4_list_h' (list) to file 'y_pred_4_list_h.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_4_h = gnb.fit(Four_Features_H, Target_Array).predict(Four_Features_Test_H)\n",
    "y_pred_4_list_h = y_pred_4_h.tolist()\n",
    "%store y_pred_4_list_h > y_pred_4_list_h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5085\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Four_Features_Test_H.shape[0],(Target_Array_Test != y_pred_4_h).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4799\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 5025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4884975570032573\n"
     ]
    }
   ],
   "source": [
    "print (4799./9824.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_4_h' (list) to file 'y_pred_list_svm_4_h.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_4_svm_h = clf.fit(Four_Features_H, Target_Array).predict(Four_Features_Test_H)\n",
    "y_pred_list_svm_4_h = y_pred_4_svm_h.tolist()\n",
    "%store y_pred_list_svm_4_h > y_pred_list_svm_4_h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5401\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Four_Features_Test_H.shape[0],(Target_Array_Test != y_pred_4_svm_h).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_lr_4_h' (list) to file 'y_pred_list_lr_4_h.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_4_lr_h = clf.fit(Four_Features_H, Target_Array).predict(Four_Features_Test_H)\n",
    "y_pred_list_lr_4_h = y_pred_4_lr_h.tolist()\n",
    "%store y_pred_list_lr_4_h > y_pred_list_lr_4_h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5052\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Four_Features_Test_H.shape[0],(Target_Array_Test != y_pred_4_lr_h).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_Features_M = np.concatenate((Three_Features, Match_Array), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_Features_Test_M = np.concatenate((Three_Features_Test, Match_Array_Test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_4_list_m' (list) to file 'y_pred_4_list_m.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_4_m = gnb.fit(Four_Features_H, Target_Array).predict(Four_Features_Test_H)\n",
    "y_pred_4_list_m = y_pred_4_m.tolist()\n",
    "%store y_pred_4_list_m > y_pred_4_list_m.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5085\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Four_Features_Test_M.shape[0],(Target_Array_Test != y_pred_4_m).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_svm_4_m' (list) to file 'y_pred_list_svm_4_m.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_4_svm_m = clf.fit(Four_Features_M, Target_Array).predict(Four_Features_Test_M)\n",
    "y_pred_list_svm_4_m = y_pred_4_svm_m.tolist()\n",
    "%store y_pred_list_svm_4_m > y_pred_list_svm_4_m.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5411\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Four_Features_Test_M.shape[0],(Target_Array_Test != y_pred_4_svm_m).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_lr_4_m' (list) to file 'y_pred_list_lr_4_m.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_4_lr_m = clf.fit(Four_Features_M, Target_Array).predict(Four_Features_Test_M)\n",
    "y_pred_list_lr_4_m = y_pred_4_lr_m.tolist()\n",
    "%store y_pred_list_lr_4_m > y_pred_list_lr_4_m.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 5236\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Four_Features_Test_H.shape[0],(Target_Array_Test != y_pred_4_lr_m).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "wordCount = defaultdict(int)\n",
    "for j in range(len(pp.data)):\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    for w in hypothesis:\n",
    "        wordCount[w]+=1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store wordCount > wordCount.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man', 'peopl', 'woman', 'two', 'play', 'girl', 'boy', 'dog', 'men', 'sit', 'wear', 'person', 'walk', 'stand', 'group', 'women', 'young', 'child', 'outside.', 'ride', 'look', 'hold', 'run', 'three', 'children', 'watch', 'kid', 'outsid', 'eat', 'take', 'get', 'work', 'black', 'near', 'wait', 'jump', 'player', 'white', 'ladi', 'front', 'coupl', 'red', 'one', 'go', 'blue', 'guy', 'dress', 'perform', 'littl', 'shirt', 'sleep', 'street.', 'make', 'bike', 'old', 'street', 'talk', 'tri', 'water.', 'worker', 'swim', 'outdoors.', 'beach.', 'next', 'someon', 'car', 'crowd', 'water', 'pictur', 'insid', 'use', 'around', 'park.', 'danc', 'enjoy', 'friend', 'human', 'drink', 'babi', 'larg', 'read', 'four', 'ball', 'small', 'game', 'build', 'park', 'carri', 'cat', 'soccer', 'game.', 'green', 'race', 'together.', 'drive', 'pose', 'climb', 'anim', 'famili', 'prepar']\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocabulary\n",
    "#wordCount = eval(open(\"wordCount.txt\").read())\n",
    "uni_sorted = sorted(wordCount.items(), key=operator.itemgetter(1)) [::-1]\n",
    "vocab = [item[0] for item in uni_sorted[:10000]]\n",
    "print (vocab[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print (len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "Char_Vector_List = []\n",
    "for j in range(len(pp.data)):\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    L = []\n",
    "    for w in hypothesis:\n",
    "        if w in vocab:\n",
    "            index = vocab.index(w)\n",
    "            L.append(index)\n",
    "    Char_Vector_List.append(L)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Char_Vector_List' (list) to file 'Char_Vector_List.txt'.\n"
     ]
    }
   ],
   "source": [
    "%store Char_Vector_List > Char_Vector_List.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 148, 108, 306]\n"
     ]
    }
   ],
   "source": [
    "print (Char_Vector_List[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n"
     ]
    }
   ],
   "source": [
    "print (len(Char_Vector_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "Char_Vector_List = eval(open(\"Char_Vector_List.txt\").read())\n",
    "indptr = [0]\n",
    "length = 0\n",
    "indices = []\n",
    "for d in Char_Vector_List:\n",
    "    length = length + len(d)\n",
    "    indptr.append(length)\n",
    "    for j in range(len(d)):\n",
    "        indices.append(d[j])\n",
    "data = np.ones(length)        \n",
    "X = csr_matrix((data, indices, indptr), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 10000)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print (X.shape)\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "Three_Features_Sparse = csr_matrix(Three_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "Many_Features = hstack([Three_Features_Sparse,X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 10003)\n"
     ]
    }
   ],
   "source": [
    "print (Many_Features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "Char_Vector_Array_Test = np.zeros((len(tt.data),len(vocab)))\n",
    "for j in range(len(tt.data)):\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    L = []\n",
    "    for w in hypothesis:\n",
    "        if w in vocab:\n",
    "            index = vocab.index(w)\n",
    "            Char_Vector_Array_Test[j,index] = 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9824, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (Char_Vector_Array_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "Many_Features_Test = np.concatenate((Three_Features_Test, Char_Vector_Array_Test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9824, 10003)\n"
     ]
    }
   ],
   "source": [
    "print (Many_Features_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\amulya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_many_lr_train' (list) to file 'y_pred_list_many_lr_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_many_lr_train = clf.fit(Many_Features, Target_Array).predict(Many_Features)\n",
    "y_pred_list_many_lr_train = y_pred_many_lr_train.tolist()\n",
    "%store y_pred_list_many_lr_train > y_pred_list_many_lr_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 184674\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Many_Features.shape[0],(Target_Array != y_pred_many_lr_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364820\n"
     ]
    }
   ],
   "source": [
    "print (549367 - 184547)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6640733789980104\n"
     ]
    }
   ],
   "source": [
    "print (364820./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_many_lr' (list) to file 'y_pred_list_many_lr.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_many_lr = clf.fit(Many_Features, Target_Array).predict(Many_Features_Test)\n",
    "y_pred_list_many_lr = y_pred_many_lr.tolist()\n",
    "%store y_pred_list_many_lr > y_pred_list_many_lr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 3290\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Many_Features_Test.shape[0],(Target_Array_Test != y_pred_many_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6523\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 3301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6639861563517915\n"
     ]
    }
   ],
   "source": [
    "print (6523./9824)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.65      0.67      0.66      3237\n",
      "   entailment       0.67      0.74      0.70      3368\n",
      "      neutral       0.68      0.59      0.63      3219\n",
      "\n",
      "     accuracy                           0.67      9824\n",
      "    macro avg       0.67      0.66      0.66      9824\n",
      " weighted avg       0.67      0.67      0.66      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Target_Array_Test, y_pred_many_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2160,  587,  490],\n",
       "       [ 468, 2486,  414],\n",
       "       [ 675,  656, 1888]], dtype=int64)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Target_Array_Test, y_pred_many_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_many_svm_train' (list) to file 'y_pred_list_many_svm_train.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_many_svm_train = clf.fit(Many_Features, Target_Array).predict(Many_Features)\n",
    "y_pred_list_many_svm_train = y_pred_many_svm_train.tolist()\n",
    "%store y_pred_list_many_svm_train > y_pred_list_many_svm_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 549367 points : 209330\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Many_Features.shape[0],(Target_Array != y_pred_many_svm_train).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345657\n"
     ]
    }
   ],
   "source": [
    "print (549367 - 203710)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6291914148465416\n"
     ]
    }
   ],
   "source": [
    "print (345657./549367.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_many_svm' (list) to file 'y_pred_list_many_svm.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_many_svm = clf.fit(Many_Features, Target_Array).predict(Many_Features_Test)\n",
    "y_pred_list_many_svm = y_pred_many_svm.tolist()\n",
    "%store y_pred_list_many_svm > y_pred_list_many_svm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 3560\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Many_Features_Test.shape[0],(Target_Array_Test != y_pred_many_svm).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5986\n"
     ]
    }
   ],
   "source": [
    "print (9824 - 3838)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6093241042345277\n"
     ]
    }
   ],
   "source": [
    "print (5986./9824.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### New code Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'y_pred_list_many_rr' (list) to file 'y_pred_list_many_rr.txt'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "# Regularized regression\n",
    "clf = linear_model.RidgeClassifier(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "y_pred_many_rr = clf.fit(Many_Features, Target_Array).predict(Many_Features_Test)\n",
    "y_pred_list_many_rr = y_pred_many_rr.tolist()\n",
    "%store y_pred_list_many_rr > y_pred_list_many_rr.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 9824 points : 3354\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Many_Features_Test.shape[0],(Target_Array_Test != y_pred_many_rr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6513\n"
     ]
    }
   ],
   "source": [
    "print(9824 - 3311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6629682410423453\n"
     ]
    }
   ],
   "source": [
    "print(6513/9824)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity + Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "vectorizer = CountVectorizer()\n",
    "cosine_sim_vector = []\n",
    "\n",
    "#all_words = defaultdict(int)\n",
    "#prem_hyp_pair = []\n",
    "X =[]\n",
    "for j in range(len(pp.data)):\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1)\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    #for word in premise:\n",
    "    #    all_words[word] += 1\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    #for word in hypothesis:\n",
    "    #    all_words[word] += 1\n",
    "    allsentences = []\n",
    "    allsentences.append(' '.join(premise))\n",
    "    allsentences.append(' '.join(hypothesis))\n",
    "    #prem_hyp_pair.append((premise,hypothesis))\n",
    "    \n",
    "    sparse_matrix = vectorizer.fit_transform(allsentences)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    \n",
    "    cosine_sim_vector.append(cosine_similarity(doc_term_matrix, doc_term_matrix)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine_Sim_Array = np.array(cosine_sim_vector)\n",
    "Cosine_Sim_Array = Cosine_Sim_Array[:, np.newaxis]\n",
    "\n",
    "Cos_Three_Features = np.concatenate((Two_Features, Cosine_Sim_Array), axis=1)\n",
    "print(Cos_Three_Features.shape)\n",
    "print(Cosine_Sim_Array.shape, Two_Features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "vectorizer = CountVectorizer()\n",
    "cosine_sim_vector_test = []\n",
    "\n",
    "#all_words = defaultdict(int)\n",
    "#prem_hyp_pair = []\n",
    "X =[]\n",
    "for j in range(len(tt.data)):\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1)\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    #for word in premise:\n",
    "    #    all_words[word] += 1\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    #for word in hypothesis:\n",
    "    #    all_words[word] += 1\n",
    "    allsentences = []\n",
    "    allsentences.append(' '.join(premise))\n",
    "    allsentences.append(' '.join(hypothesis))\n",
    "    #prem_hyp_pair.append((premise,hypothesis))\n",
    "    \n",
    "    sparse_matrix = vectorizer.fit_transform(allsentences)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    \n",
    "    cosine_sim_vector_test.append(cosine_similarity(doc_term_matrix, doc_term_matrix)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine_Sim_Array_Test = np.array(cosine_sim_vector_test)\n",
    "Cosine_Sim_Array_Test = Cosine_Sim_Array_Test[:, np.newaxis]\n",
    "\n",
    "Cos_Three_Features_Test = np.concatenate((Two_Features_Test, Cosine_Sim_Array_Test), axis=1)\n",
    "print(Cos_Three_Features_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_cos_three_lr = clf.fit(Cos_Three_Features, Target_Array).predict(Cos_Three_Features_Test)\n",
    "y_pred_list_cos_three_lr = y_pred_cos_three_lr.tolist()\n",
    "#%store y_pred_list_cos_three_lr > y_pred_list_cos_three_lr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = Cos_Three_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_cos_three_lr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Cos_Three_Features_Test.shape[0],(Target_Array_Test != y_pred_cos_three_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Many features along with cosine similarity\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "Cos_Many_Features = hstack([Many_Features,Cosine_Sim_Array])\n",
    "print(Many_Features.shape, Cosine_Sim_Array.shape)\n",
    "\n",
    "print(Cos_Many_Features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cos_Many_Features_Test = np.concatenate((Many_Features_Test, Cosine_Sim_Array_Test), axis=1)\n",
    "print(Cos_Many_Features_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_cos_many_lr = clf.fit(Cos_Many_Features, Target_Array).predict(Cos_Many_Features_Test)\n",
    "y_pred_list_cos_many_lr = y_pred_cos_many_lr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = Cos_Many_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_cos_many_lr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Cos_Many_Features_Test.shape[0],(Target_Array_Test != y_pred_cos_many_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "cosine_sim_vector = []\n",
    "\n",
    "#all_words = defaultdict(int)\n",
    "#prem_hyp_pair = []\n",
    "for j in range(len(pp.data)):\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1)\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    #for word in premise:\n",
    "    #    all_words[word] += 1\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    #for word in hypothesis:\n",
    "    #    all_words[word] += 1\n",
    "    allsentences = []\n",
    "    allsentences.append(' '.join(premise))\n",
    "    allsentences.append(' '.join(hypothesis))\n",
    "    #prem_hyp_pair.append((premise,hypothesis))\n",
    "    \n",
    "    sparse_matrix = vectorizer.fit_transform(allsentences)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    \n",
    "    cosine_sim_vector.append(cosine_similarity(doc_term_matrix, doc_term_matrix)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine_Sim_Array = np.array(cosine_sim_vector)\n",
    "Cosine_Sim_Array = Cosine_Sim_Array[:, np.newaxis]\n",
    "\n",
    "Cos_Three_Features = np.concatenate((Two_Features, Cosine_Sim_Array), axis=1)\n",
    "print(Cos_Three_Features.shape)\n",
    "print(Cosine_Sim_Array.shape, Two_Features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "cosine_sim_vector_test = []\n",
    "\n",
    "#all_words = defaultdict(int)\n",
    "#prem_hyp_pair = []\n",
    "X =[]\n",
    "for j in range(len(tt.data)):\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1)\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    #for word in premise:\n",
    "    #    all_words[word] += 1\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    #for word in hypothesis:\n",
    "    #    all_words[word] += 1\n",
    "    allsentences = []\n",
    "    allsentences.append(' '.join(premise))\n",
    "    allsentences.append(' '.join(hypothesis))\n",
    "    #prem_hyp_pair.append((premise,hypothesis))\n",
    "    \n",
    "    sparse_matrix = vectorizer.fit_transform(allsentences)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    \n",
    "    cosine_sim_vector_test.append(cosine_similarity(doc_term_matrix, doc_term_matrix)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine_Sim_Array_Test = np.array(cosine_sim_vector_test)\n",
    "Cosine_Sim_Array_Test = Cosine_Sim_Array_Test[:, np.newaxis]\n",
    "\n",
    "Cos_Three_Features_Test = np.concatenate((Two_Features_Test, Cosine_Sim_Array_Test), axis=1)\n",
    "print(Cos_Three_Features_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_cos_three_lr = clf.fit(Cos_Three_Features, Target_Array).predict(Cos_Three_Features_Test)\n",
    "y_pred_list_cos_three_lr = y_pred_cos_three_lr.tolist()\n",
    "#%store y_pred_list_cos_three_lr > y_pred_list_cos_three_lr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = Cos_Three_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_cos_three_lr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Cos_Three_Features_Test.shape[0],(Target_Array_Test != y_pred_cos_three_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Many features along with cosine similarity\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "Cos_Many_Features = hstack([Many_Features,Cosine_Sim_Array])\n",
    "print(Many_Features.shape, Cosine_Sim_Array.shape)\n",
    "\n",
    "print(Cos_Many_Features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cos_Many_Features_Test = np.concatenate((Many_Features_Test, Cosine_Sim_Array_Test), axis=1)\n",
    "print(Cos_Many_Features_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_cos_many_lr = clf.fit(Cos_Many_Features, Target_Array).predict(Cos_Many_Features_Test)\n",
    "y_pred_list_cos_many_lr = y_pred_cos_many_lr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = Cos_Many_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_cos_many_lr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Cos_Many_Features_Test.shape[0],(Target_Array_Test != y_pred_cos_many_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# Regularized regression\n",
    "clf = linear_model.RidgeClassifier(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "y_pred_cos_three_rr = clf.fit(Cos_Three_Features, Target_Array).predict(Cos_Three_Features_Test)\n",
    "y_pred_list_cos_three_rr = y_pred_cos_three_rr.tolist()\n",
    "#%store y_pred_list_cos_three_lr > y_pred_list_cos_three_lr.txt\n",
    "\n",
    "total = Cos_Three_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_cos_three_rr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Cos_Three_Features_Test.shape[0],(Target_Array_Test != y_pred_cos_three_rr).sum()))\n",
    "\n",
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))\n",
    "\n",
    "### Using Many features along with cosine similarity\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "Cos_Many_Features = hstack([Many_Features,Cosine_Sim_Array])\n",
    "print(Many_Features.shape, Cosine_Sim_Array.shape)\n",
    "\n",
    "print(Cos_Many_Features.shape)\n",
    "\n",
    "Cos_Many_Features_Test = np.concatenate((Many_Features_Test, Cosine_Sim_Array_Test), axis=1)\n",
    "print(Cos_Many_Features_Test.shape)\n",
    "\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.RidgeClassifier(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "y_pred_cos_many_rr = clf.fit(Cos_Many_Features, Target_Array).predict(Cos_Many_Features_Test)\n",
    "y_pred_list_cos_many_rr = y_pred_cos_many_rr.tolist()\n",
    "\n",
    "total = Cos_Many_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_cos_many_rr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Cos_Many_Features_Test.shape[0],(Target_Array_Test != y_pred_cos_many_rr).sum()))\n",
    "\n",
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_cos_three_svm = clf.fit(Cos_Three_Features, Target_Array).predict(Cos_Three_Features_Test)\n",
    "y_pred_list_cos_three_svm = y_pred_cos_three_svm.tolist()\n",
    "#%store y_pred_list_cos_three_lr > y_pred_list_cos_three_lr.txt\n",
    "\n",
    "total = Cos_Three_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_cos_three_svm).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Cos_Three_Features_Test.shape[0],(Target_Array_Test != y_pred_cos_three_svm).sum()))\n",
    "\n",
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))\n",
    "\n",
    "### Using Many features along with cosine similarity\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "Cos_Many_Features = hstack([Many_Features,Cosine_Sim_Array])\n",
    "print(Many_Features.shape, Cosine_Sim_Array.shape)\n",
    "\n",
    "print(Cos_Many_Features.shape)\n",
    "\n",
    "Cos_Many_Features_Test = np.concatenate((Many_Features_Test, Cosine_Sim_Array_Test), axis=1)\n",
    "print(Cos_Many_Features_Test.shape)\n",
    "\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "y_pred_cos_many_svm = clf.fit(Cos_Many_Features, Target_Array).predict(Cos_Many_Features_Test)\n",
    "y_pred_list_cos_many_svm = y_pred_cos_many_svm.tolist()\n",
    "\n",
    "total = Cos_Many_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_cos_many_svm).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Cos_Many_Features_Test.shape[0],(Target_Array_Test != y_pred_cos_many_svm).sum()))\n",
    "\n",
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "euclid_sim_vector = []\n",
    "\n",
    "#all_words = defaultdict(int)\n",
    "#prem_hyp_pair = []\n",
    "for j in range(len(pp.data)):\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1)\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    #for word in premise:\n",
    "    #    all_words[word] += 1\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    #for word in hypothesis:\n",
    "    #    all_words[word] += 1\n",
    "    allsentences = []\n",
    "    allsentences.append(' '.join(premise))\n",
    "    allsentences.append(' '.join(hypothesis))\n",
    "    #prem_hyp_pair.append((premise,hypothesis))\n",
    "    \n",
    "    sparse_matrix = vectorizer.fit_transform(allsentences)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    \n",
    "    euclid_sim_vector.append(euclidean_distances(doc_term_matrix, doc_term_matrix)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclid_Sim_Array = np.array(euclid_sim_vector)\n",
    "Euclid_Sim_Array = Euclid_Sim_Array[:, np.newaxis]\n",
    "\n",
    "Euc_Three_Features = np.concatenate((Two_Features, Euclid_Sim_Array), axis=1)\n",
    "print(Euc_Three_Features.shape)\n",
    "print(Euclid_Sim_Array.shape, Two_Features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "euclid_sim_vector_test = []\n",
    "\n",
    "#all_words = defaultdict(int)\n",
    "#prem_hyp_pair = []\n",
    "X =[]\n",
    "for j in range(len(tt.data)):\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1)\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    #for word in premise:\n",
    "    #    all_words[word] += 1\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    #for word in hypothesis:\n",
    "    #    all_words[word] += 1\n",
    "    allsentences = []\n",
    "    allsentences.append(' '.join(premise))\n",
    "    allsentences.append(' '.join(hypothesis))\n",
    "    #prem_hyp_pair.append((premise,hypothesis))\n",
    "    \n",
    "    sparse_matrix = vectorizer.fit_transform(allsentences)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    \n",
    "    euclid_sim_vector_test.append(euclidean_distances(doc_term_matrix, doc_term_matrix)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclid_Sim_Array_Test = np.array(euclid_sim_vector_test)\n",
    "Euclid_Sim_Array_Test = Euclid_Sim_Array_Test[:, np.newaxis]\n",
    "\n",
    "Euc_Three_Features_Test = np.concatenate((Two_Features_Test, Euclid_Sim_Array_Test), axis=1)\n",
    "print(Euc_Three_Features_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_euc_three_lr = clf.fit(Euc_Three_Features, Target_Array).predict(Euc_Three_Features_Test)\n",
    "#y_pred_list_cos_three_lr = y_pred_cos_three_lr.tolist()\n",
    "#%store y_pred_list_cos_three_lr > y_pred_list_cos_three_lr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = Euc_Three_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_euc_three_lr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Euc_Three_Features_Test.shape[0],(Target_Array_Test != y_pred_euc_three_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Many features along with cosine similarity\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "Euc_Many_Features = hstack([Many_Features,Euclid_Sim_Array])\n",
    "print(Many_Features.shape, Euclid_Sim_Array.shape)\n",
    "\n",
    "print(Cos_Many_Features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Euc_Many_Features_Test = np.concatenate((Many_Features_Test, Euclid_Sim_Array_Test), axis=1)\n",
    "print(Euc_Many_Features_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_euc_many_lr = clf.fit(Euc_Many_Features, Target_Array).predict(Euc_Many_Features_Test)\n",
    "y_pred_list_euc_many_lr = y_pred_euc_many_lr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = Euc_Many_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_euc_many_lr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Euc_Many_Features_Test.shape[0],(Target_Array_Test != y_pred_euc_many_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard on pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(A,B):\n",
    "    return len(A.intersection(B)) / len(A.union(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "jaccard_scores = []\n",
    "\n",
    "#all_words = defaultdict(int)\n",
    "#prem_hyp_pair = []\n",
    "for j in range(len(pp.data)):\n",
    "    sentence1 = pp.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1)\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    #for word in premise:\n",
    "    #    all_words[word] += 1\n",
    "    sentence2 = pp.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    jaccard_scores.append(Jaccard(set(premise), set(hypothesis)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "jaccard_scores_test =[]\n",
    "\n",
    "for j in range(len(tt.data)):\n",
    "    sentence1 = tt.data[j]['sentence1']\n",
    "    sentence1 = unicodedata.normalize('NFKD', sentence1)\n",
    "    premise = [ps.stem(i) for i in sentence1.lower().split() if i not in stop]\n",
    "    sentence2 = tt.data[j]['sentence2']\n",
    "    sentence2 = unicodedata.normalize('NFKD', sentence2)\n",
    "    hypothesis = [ps.stem(i) for i in sentence2.lower().split() if i not in stop]\n",
    "    jaccard_scores_test.append(Jaccard(set(premise), set(hypothesis)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sim_Array = np.array(jaccard_scores)\n",
    "Sim_Array = Sim_Array[:, np.newaxis]\n",
    "\n",
    "Jacc_Three_Features = np.concatenate((Two_Features, Sim_Array), axis=1)\n",
    "Jacc_Many_Features = hstack([Many_Features, Sim_Array])\n",
    "\n",
    "Sim_Array_Test = np.array(jaccard_scores_test)\n",
    "Sim_Array_Test = Sim_Array_Test[:, np.newaxis]\n",
    "print(Sim_Array_Test.shape)\n",
    "\n",
    "Jacc_Three_Features_Test = np.concatenate((Two_Features_Test, Sim_Array_Test), axis=1)\n",
    "Jacc_Many_Features_Test = np.concatenate((Many_Features_Test, Sim_Array_Test), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_jacc_three_lr = clf.fit(Jacc_Three_Features, Target_Array).predict(Jacc_Three_Features_Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = Jacc_Three_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_jacc_three_lr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Jacc_Three_Features_Test.shape[0],(Target_Array_Test != y_pred_jacc_three_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "y_pred_jacc_many_lr = clf.fit(Jacc_Many_Features, Target_Array).predict(Jacc_Many_Features_Test)\n",
    "\n",
    "\n",
    "total = Jacc_Many_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_jacc_many_lr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Jacc_Many_Features_Test.shape[0],(Target_Array_Test != y_pred_jacc_many_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Jacc_Cos_Many_Features = hstack([Jacc_Many_Features, Cosine_Sim_Array])\n",
    "Jacc_Cos_Many_Features_Test = np.concatenate((Jacc_Many_Features_Test, Cosine_Sim_Array_Test), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression(C=0.1)\n",
    "y_pred_jacc_cos_many_lr = clf.fit(Jacc_Cos_Many_Features, Target_Array).predict(Jacc_Cos_Many_Features_Test)\n",
    "\n",
    "\n",
    "total = Jacc_Cos_Many_Features_Test.shape[0]\n",
    "mislabelled = (Target_Array_Test != y_pred_jacc_cos_many_lr).sum()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (Jacc_Cos_Many_Features_Test.shape[0],(Target_Array_Test != y_pred_jacc_cos_many_lr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (total-mislabelled) / total\n",
    "print(\"Accuracy {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
